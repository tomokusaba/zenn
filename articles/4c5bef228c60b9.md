---
title: "M5Stack CoreS3 と Azure AI Foundry で音声アシスタント「スタックチャン AI」を作った話 — リレーサーバーで途切れ問題を解決する"
emoji: "🤖"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["M5Stack", "ESP32", "AzureOpenAI", "dotnet", "IoT"]
published: false
---

## はじめに

M5Stack CoreS3（ESP32-S3）と Azure AI Foundry の GPT Realtime API を組み合わせて、音声で会話できるロボットアシスタント「スタックチャン AI」を開発しました🔧

Azure AI Foundry（旧 Azure OpenAI）が提供する GPT Realtime API は、音声入力をリアルタイムに受け取り、音声で応答を返す「speech in, speech out」型の対話を低レイテンシで実現できる API です。WebSocket 経由で 24kHz の PCM 音声をストリーミングし、GPT-4o ファミリーのモデルがリアルタイムに応答を生成します🎙️

この記事では、ESP32 から Azure の Realtime API に直接アクセスした際に発生した「音声が途切れ途切れになる」問題と、それを中継サーバー（リレーサーバー）の導入によって解決したアーキテクチャについて、実装コードを交えながら解説します📝

## システムの全体像

最終的なシステム構成は以下のとおりです。ESP32 と Azure の間に C#/.NET 10 で実装したリレーサーバーを挟み、ESP32 側の処理負荷を大幅に軽減しています🏗️

```
M5Stack CoreS3 (ESP32-S3)
    │  plain TCP (LAN内バイナリプロトコル)
    ▼
リレーサーバー (PC / .NET 8)
    │  WSS/TLS (Azure OpenAI Realtime API)
    ▼
Azure AI Foundry — GPT Realtime API (gpt-realtime-mini)
```

この構成に至るまでには、ESP32 から Azure に直接接続するアプローチを試み、実用上の大きな課題にぶつかるという過程がありました。以降では、その課題と解決策を順を追って説明します🔍

## 直接接続で発生した問題

### ESP32 から Azure Realtime API に直接接続する構成

当初の構成では、ESP32 が Wi-Fi 経由で直接 Azure OpenAI の WebSocket エンドポイント（`wss://` URL）に接続し、音声の送受信をすべて ESP32 単体で行っていました。Azure Realtime API は WebSocket 上で JSON メッセージをやり取りする設計になっており、音声データは Base64 エンコードされた文字列として JSON に埋め込まれます🔗

具体的には、マイクから取得した 24kHz/16bit の PCM 音声を Base64 にエンコードし、`input_audio_buffer.append` イベントの `audio` フィールドに格納して送信します。Azure からの応答音声も同様に、`response.audio.delta` イベントの `delta` フィールドに Base64 文字列として返されます📡

### 途切れ途切れの応答音声

この構成を実際に動かしてみると、Azure からの応答音声が著しく途切れ途切れになるという現象が発生しました。文章の途中で音声が途絶え、数百ミリ秒の無音が挟まり、また途中から再開するという状態です😰

原因を分析すると、ESP32-S3 にとって以下の処理が同時に走ることが過負荷であると判明しました🔥

まず **TLS ハンドシェイクと暗号化通信** の負荷があります。Azure の WebSocket エンドポイントは `wss://` プロトコル、つまり TLS で暗号化された WebSocket を使用します。ESP32-S3 は TLS をソフトウェア実装で処理するため、暗号化・復号のたびに CPU サイクルを大量に消費します。特に音声ストリーミング中は数十ミリ秒間隔で連続的にデータが到着するため、TLS の復号処理がボトルネックになります🔒

次に **Base64 エンコード/デコード** の負荷です。Azure Realtime API は音声データを JSON 内の Base64 文字列として送受信します。100ms 分の 24kHz/16bit PCM 音声は 4,800 バイトで、Base64 にエンコードすると約 6,400 バイトに膨張します。このエンコード・デコード処理自体は単純な計算ですが、ストリーミング中に連続して行うと CPU 時間を圧迫します📊

さらに **JSON のパースと構築** の負荷も無視できません。Azure から受信する各イベントは JSON テキストメッセージであり、音声データを含む `response.audio.delta` イベントは数キロバイトから数十キロバイトの JSON 文字列になります。ESP32 上でこの JSON をパースして `delta` フィールドを抽出し、Base64 デコードするという一連の処理は、RAM が限られた組み込み環境では特に重い処理です🧮

これらの処理が重なった結果、ESP32 の loop 処理が間に合わず、WebSocket の受信バッファが溢れたり、次の音声チャンクの受信が遅延したりして、再生時に音飛びが発生していました。同じマイクロコントローラ上でマイクの録音（I2S DMA）、スピーカーの再生（I2S DMA）、Wi-Fi 通信、そして上記の重い処理がすべて同時に動くため、FreeRTOS のタスクスケジューリングでも完全にはカバーしきれない状態でした⚡

### 音声認識の精度低下

応答音声の途切れに加えて、音声認識（マイク入力のトランスクリプション）の精度も非常に悪いという問題がありました。こちらが話した内容が正しく認識されず、的外れな応答が返ってくることが頻発していました🤔

この原因は、送信側でも同様の処理負荷が発生していたためと考えられます。マイクから取得した PCM 音声を Base64 にエンコードし、JSON に包んで TLS 暗号化して送信するという一連の処理に時間がかかり、音声チャンクの送信間隔が不均一になっていました。その結果、Azure 側が受け取る音声データにギャップが生じ、Whisper による音声認識の精度が著しく低下していたのです🎤

## リレーサーバーによる解決

### 設計思想

これらの問題を解決するために、ESP32 と Azure の間にリレーサーバー（中継サーバー）を配置するアーキテクチャに移行しました。設計方針は明確で、「ESP32 がやる必要のない処理はすべて PC 側にオフロードする」というものです💡

ESP32 は LAN 内のリレーサーバーにプレーン TCP（非暗号化）でバイナリデータを送受信するだけにとどめ、TLS 接続、Base64 エンコード/デコード、JSON の構築・パースといった重い処理はすべてリレーサーバーが担当します。LAN 内通信であるため、TLS による暗号化は不要です。ESP32 からは生の PCM 音声バイト列をそのまま送信し、リレーサーバーがそれを受け取って Base64 エンコードと JSON ラッピングを行ってから Azure に転送します。逆方向も同様に、Azure から受信した JSON 内の Base64 音声データをリレーサーバーがデコードし、生の PCM バイト列として ESP32 に送り返します🔄

### バイナリプロトコルの設計

ESP32 とリレーサーバーの間では、独自のバイナリフレームプロトコルを使用しています。各フレームは 5 バイトのヘッダとペイロードで構成されます📦

```
[1バイト: メッセージタイプ][4バイト: ペイロード長 (ビッグエンディアン)][ペイロード]
```

ESP32 からリレーサーバーへの上り方向では、タイプ `0x01` が JSON テキスト（`session.update` や `response.create` など制御メッセージ）、タイプ `0x02` が Raw PCM 音声データを意味します。リレーサーバーから ESP32 への下り方向では、タイプ `0x01` がデコード済みの Raw PCM 音声データ、タイプ `0x02` が JSON イベント（音声以外の全イベント）を意味します🎯

ESP32 側でのフレーム送信処理は非常にシンプルです。5 バイトのヘッダを構築して TCP ソケットに書き込み、続けてペイロードを書き込むだけで、Base64 エンコードも JSON 構築も不要です📤

```cpp
bool sendBinFrame(uint8_t type, const uint8_t *data, uint32_t len) {
    if (!wsConnected || !tcpClient.connected()) return false;
    uint8_t header[5];
    header[0] = type;
    header[1] = (len >> 24) & 0xFF;
    header[2] = (len >> 16) & 0xFF;
    header[3] = (len >> 8) & 0xFF;
    header[4] = len & 0xFF;
    tcpClient.write(header, 5);
    if (len > 0 && data) {
        tcpClient.write(data, len);
    }
    return true;
}
```

### リレーサーバーの実装（C#/.NET 10）

リレーサーバーは C#/.NET 10 のコンソールアプリケーションとして実装しました。起動すると TCP リスナーで ESP32 からの接続を待ち受け、接続を受け付けると Azure OpenAI の WebSocket エンドポイントに WSS 接続を確立します。以降は上り（ESP32 → Azure）と下り（Azure → ESP32）の 2 つの非同期タスクを並行して実行し、データの中継を行います🖥️

上り方向の処理では、ESP32 から受信した Raw PCM 音声を Base64 にエンコードし、`input_audio_buffer.append` イベントとして JSON にラップして Azure に送信しています✈️

```csharp
case UpAudio:
{
    var b64 = Convert.ToBase64String(payload);
    var json = JsonSerializer.Serialize(new
    {
        type = "input_audio_buffer.append",
        audio = b64,
    });
    await ws.SendAsync(
        Encoding.UTF8.GetBytes(json),
        WebSocketMessageType.Text, true, token);
    audioChunks++;
    break;
}
```

下り方向の処理では、Azure から受信した `response.audio.delta` イベント内の Base64 音声データをデコードし、生の PCM バイト列としてバイナリフレームに包んで ESP32 に送信します🔊

```csharp
if (isAudioDelta)
{
    using var doc = JsonDocument.Parse(message);
    var root = doc.RootElement;
    if (root.TryGetProperty("delta", out var deltaProp))
    {
        var b64Audio = deltaProp.GetString();
        if (!string.IsNullOrEmpty(b64Audio))
        {
            var pcm = Convert.FromBase64String(b64Audio);
            // DOWN_AUDIO フレーム送信
            var frame = new byte[5 + pcm.Length];
            frame[0] = DownAudio;
            BinaryPrimitives.WriteUInt32BigEndian(
                frame.AsSpan(1), (uint)pcm.Length);
            pcm.CopyTo(frame, 5);

            await writeLock.WaitAsync(token);
            try
            {
                await stream.WriteAsync(frame, token);
                await stream.FlushAsync(token);
            }
            finally { writeLock.Release(); }
        }
    }
}
```

TCP の書き込みは `SemaphoreSlim` で直列化しており、上り方向と下り方向のフレームが混在しても破損しないようにしています🛡️

## ESP32 側の FreeRTOS アーキテクチャ

リレーサーバーの導入によって ESP32 側の処理が軽量化されたため、FreeRTOS のマルチタスクを活用した効率的なアーキテクチャを実現できました🏗️

### PSRAM を活用したメモリ管理

M5Stack CoreS3 に搭載されている ESP32-S3 は 8MB の PSRAM を備えています。この大容量メモリを活かし、マイク音声用に 10 秒分のリングバッファ（480KB）と、応答音声の蓄積用に 1.5MB のバッファを PSRAM 上に確保しています🧠

```cpp
static const size_t MIC_RING_SAMPLES = SAMPLE_RATE * 10; // 24000 * 10 = 240000サンプル
static const size_t AUDIO_BUF_SIZE = 1536000;             // 1.5MB（約32秒@24kHz）

micRingBuf = (int16_t *)ps_malloc(MIC_RING_SAMPLES * sizeof(int16_t));
audioBuf   = (uint8_t *)ps_malloc(AUDIO_BUF_SIZE);
```

### タスク構成

メインの `loop()` はマイクの録音とバイナリフレームの受信を担当し、Core 1 で動作します。音声送信タスク `sendAudioTask` はリングバッファから PCM データを読み出してリレーサーバーに送信する専用タスクで、Core 0 にピン留めされています。音声再生監視タスク `audioPlaybackTask` も Core 0 で動作し、Azure からの応答音声がすべて蓄積された後に `M5.Speaker.playRaw()` で一括再生します🔀

マイク録音タスクと音声送信タスクが分離されていることで、録音の取りこぼしなく連続的に音声データを Azure に送信できます。リングバッファを介してデータを受け渡すことで、タスク間のデカップリングを実現しています🎵

### ローカル VAD（Voice Activity Detection）

Azure Realtime API には `server_vad`（サーバー側 VAD）オプションが存在しますが、本実装では `turn_detection: null` を設定してサーバー VAD を無効化し、ESP32 上でローカル VAD を実装しています🗣️

ローカル VAD にした理由は、リレーサーバーを挟む構成ではサーバー VAD のレイテンシ制御が難しくなるためです。ESP32 上で音声のピークレベルと平均レベルを監視し、ノイズフロアの自動キャリブレーション（最初の 2 秒間）を経て、発話の開始と終了を検出しています。発話開始の判定には 400ms の連続音声検出を、発話終了の判定には 1.6 秒の連続無音を条件としています🎚️

```cpp
static const uint16_t VAD_START_FRAMES      = 4;   // 400 ms 連続で発話開始
static const uint16_t VAD_MIN_SPEECH_FRAMES = 10;  // 最低 1 秒の発話
static const uint16_t VAD_END_SILENCE_FRAMES = 16; // 1.6 秒の無音で発話終了
```

発話終了を検出すると、送信タスクをフラッシュしてリングバッファ内の残りデータを送り切り、その後 `input_audio_buffer.commit` と `response.create` を Azure に送信して応答生成をトリガーします📩

### DC オフセット除去とソフト AGC

マイク入力の品質を向上させるために、DC オフセット除去用の 1 次 IIR ハイパスフィルターとソフトウェア AGC（Automatic Gain Control）を実装しています。DC オフセットフィルターはマイクの DC バイアスを除去し、AGC はピークレベルが目標値（8000）に近づくようにゲインを自動調整します。ゲインの変化は急激にならないよう、指数移動平均でスムージングしています🎛️

```cpp
void removeDCOffset(int16_t *buf, size_t n) {
    for (size_t i = 0; i < n; i++) {
        int32_t x = buf[i];
        int32_t y = ((hpfPrevY * 255) + ((x - hpfPrevX) << 8)) >> 8;
        hpfPrevX = x;
        hpfPrevY = y;
        if (y > 32767)  y = 32767;
        if (y < -32768) y = -32768;
        buf[i] = (int16_t)y;
    }
}
```

## バージイン（再生中断）機能

ユーザーが応答再生中に画面をタッチすると、再生を即座に中断して再びリスニング状態に戻る「バージイン」機能も実装しています。内部的には `M5.Speaker.stop()` でスピーカーを停止し、Azure に `response.cancel` イベントを送信して応答生成を中断させます。音声蓄積バッファをクリアした後、通常の状態遷移フローによってリスニング状態に自動復帰します👆

## 結果

リレーサーバーの導入によって、音声の途切れ問題は完全に解消されました。ESP32 の処理が「Raw PCM の送受信のみ」に限定されたことで、CPU 負荷が大幅に低減し、マイク録音と音声再生が安定して動作するようになりました✅

音声認識の精度も劇的に改善されました。音声チャンクが途切れなく一定間隔で Azure に到達するようになったため、Whisper モデルによるトランスクリプションが正確に機能するようになっています🎉

ESP32 のようなリソースが限られた組み込みデバイスで Azure OpenAI の Realtime API を活用する場合、TLS 暗号化やデータエンコーディングといった計算コストの高い処理を外部にオフロードするリレーサーバーパターンは非常に有効なアプローチです。特に音声ストリーミングのようにリアルタイム性が求められるユースケースでは、処理の分離がシステム全体の応答品質に直結します🌟

## 動作環境

本記事の実装は以下の環境で動作確認を行いました📋

| 項目 | 内容 |
|---|---|
| デバイス | M5Stack CoreS3（ESP32-S3、PSRAM 8MB） |
| 開発環境 | PlatformIO（Arduino フレームワーク） |
| リレーサーバー | C# / .NET 10 コンソールアプリケーション |
| Azure モデル | gpt-realtime-mini |
| 音声フォーマット | PCM 24kHz / 16bit モノラル |
| API 接続方式 | WebSocket（WSS） |

## 参考文献

- [Azure OpenAI GPT Realtime API for speech and audio — Microsoft Learn](https://learn.microsoft.com/azure/ai-foundry/openai/how-to/realtime-audio?view=foundry-classic)
- [Use the GPT Realtime API via WebSockets — Microsoft Learn](https://learn.microsoft.com/azure/ai-foundry/openai/how-to/realtime-audio-websockets?view=foundry-classic)
- [GPT Realtime API クイックスタート — Microsoft Learn](https://learn.microsoft.com/azure/ai-foundry/openai/realtime-audio-quickstart?view=foundry-classic)
- [M5Unified ライブラリ — GitHub](https://github.com/m5stack/M5Unified)
- [M5Stack CoreS3 製品ページ](https://docs.m5stack.com/en/core/CoreS3)
- [FreeRTOS リファレンスマニュアル](https://www.freertos.org/Documentation/00-Overview)
